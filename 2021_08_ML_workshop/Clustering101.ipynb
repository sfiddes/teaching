{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj374cPQkLLfDoRlrW/zes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfiddes/teaching/blob/main/2021_08_ML_workshop/Clustering101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzS_njR4rots"
      },
      "source": [
        "# **Intro**\n",
        "\n",
        "\n",
        "\n",
        "This notebook steps through k-means and hierarchical clustering in python using the Palmer Penguins data set and SciKit-Learn. \n",
        "\n",
        "It has been set up to run via Google Colaboratory, so no installation on your local computer is required. Unfortunately, to take advantage of this, you will need a google account. If you do have a google account: \n",
        "- Click Open in Colab at top of notebook (if starting from GitHub)\n",
        "- Sign into google\n",
        "- Start running \n",
        "- You can Copy to Drive if you want to save any changes you make\n",
        "\n",
        "Alternatively, you can download the notebook and run it locally, if you have the required packages installed. \n",
        "- Click Open in Colab at top of notebook (if starting from GitHub)\n",
        "- File > Download as .ipynb\n",
        "- Make sure you have the right packages installed \n",
        "- Open up jupyter notebooks and start running \n",
        "\n",
        "This notebook assumes little knowledge of python. Most of the code has been hidden to ease readability, but if you want to see what the code is doing, simply hit the 'show code' buttons!  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxRi90qiqWj_"
      },
      "source": [
        "# **Set-up**\n",
        "- Install some non-default packages - the dataset Palmer Penguins. Google Colab uses pip (a package installer for python) to do this. \n",
        "\n",
        "- Import the packages that we want to use, including:\n",
        "  - Basic python packages for math/science (numpy, scipy), data handling (pandas), plotting (matplotlib, seaborn)\n",
        "  - Scikit-Learn - the home of lots of great machine learning packages. We are interested in the clustering packages, as well as some data pre-processing and metric packages\n",
        "  - Our dataset - Palmer Penguins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcyxvIVIm4uD"
      },
      "source": [
        "pip install palmerpenguins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9PDDxvN4LfH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn import preprocessing, metrics\n",
        "\n",
        "from palmerpenguins import load_penguins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8G-xWdTqx4s"
      },
      "source": [
        "#**Data**\n",
        "**Import the data and take a look at it**\n",
        "- Load the data as a Pandas DataFrame\n",
        "- Try plotting some of the data \n",
        "\n",
        "**Questions** \n",
        "- What type of data is available (categorical and/or quantitative)?\n",
        "- What will you try to predict? \n",
        "- What will you use to predict it? \n",
        "- What shape is the data set distribution? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tal1BZJfneJM"
      },
      "source": [
        "# Import our dataframe - we call it penguins\n",
        "penguins = load_penguins()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nof73_MTnfAC"
      },
      "source": [
        "# Print dataframe (can also use dataframe.head() or dataframe.tail(), or dataframe.variable to isolate columns)\n",
        "penguins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlIvJezG7Hds"
      },
      "source": [
        "**Plot a scatter plot (using seaborn)**\n",
        "- Try to plot a few different variables (eg. predictors) and categories (what we want to predict)\n",
        "- Can you find a good combination? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA_4ElrzrMPi"
      },
      "source": [
        "sns.scatterplot(data=penguins,x='flipper_length_mm',y='body_mass_g',hue='sex',palette='flare');  # change the x, y and hue variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiYSluFawA8n",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Plot a 3D scatter plot** { display-mode: \"both\" }\n",
        "c = penguins.species.copy() # need to make categorial groups quantitative [0,1,2]\n",
        "c[np.where(c=='Adelie')[0]] = 0\n",
        "c[np.where(c=='Gentoo')[0]] = 1\n",
        "c[np.where(c=='Chinstrap')[0]] = 2\n",
        "\n",
        "fig = plt.figure(figsize=(8,5)) # initialise figure\n",
        "ax = plt.axes(projection='3d')\n",
        "p = ax.scatter3D(penguins.flipper_length_mm,    # plot in 3D: x\n",
        "                 penguins.bill_length_mm,       # y\n",
        "                 penguins.bill_depth_mm,        # z\n",
        "                 c=c,                           # c (colour)\n",
        "                 cmap=sns.color_palette('flare',as_cmap=True)); \n",
        "cbar = plt.colorbar(p,drawedges=True,ticks=[0,1,2],      # add the colour bar \n",
        "                    boundaries=np.arange(-0.5,3.5,1)); \n",
        "cbar.set_ticklabels(['Adelie','Gentoo','Chinstrap']) # set color bar tick labels (so not 0,1,2)\n",
        "ax.view_init(20, 10) # make the viewing angle more optimal\n",
        "ax.set_xlabel('Flipper Length (mm)');\n",
        "ax.set_ylabel('Bill Length (mm)');\n",
        "ax.set_zlabel('Bill Depth (mm)');\n",
        "plt.title('Observed penguin data');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKgmHKO17Qlg"
      },
      "source": [
        "**How is the data distributed?**\n",
        "\n",
        "See how the data you want to use as predictors are distributed. This is important as k-means makes the assumption that data is normally distrubuted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbdS2AvZ7Qs1"
      },
      "source": [
        "# Try your own distribution plot here. Hint: \n",
        "#sns.kdeplot(data=dataframe, x=var1, hue=var2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unQ8XMjv5iPr",
        "cellView": "form"
      },
      "source": [
        "#@markdown Distribution plot example { display-mode: \"both\" }\n",
        "fig, axes = plt.subplots(1,4,figsize=(14,4))\n",
        "sns.kdeplot(data=penguins, x=\"flipper_length_mm\", hue=\"species\", ax=axes[0]); \n",
        "sns.kdeplot(data=penguins, x=\"bill_length_mm\", hue=\"species\", ax=axes[1]); \n",
        "sns.kdeplot(data=penguins, x=\"bill_depth_mm\", hue=\"species\", ax=axes[2]); \n",
        "sns.kdeplot(data=penguins, x=\"body_mass_g\", hue=\"species\", ax=axes[3]); \n",
        "plt.subplots_adjust(wspace=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXsMQhfvsnt"
      },
      "source": [
        "# **Prep your data for clustering**\n",
        "- Select the predictors (attributes) you want to use \n",
        "- Normalise the data (assuming it is normally distributed), otherwise the largest field will dominate (because we are going to minimise the Euclidean distance)\n",
        "\n",
        "**Select the attributes** \n",
        "- I've chosen three: \n",
        "\n",
        "`attributes = ['bill_length_mm','bill_depth_mm','flipper_length_mm']`\n",
        "- Have a play with different attributes, but note that you will have to also adjust some of the plots/evaluation metrics later on if you do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgA4jA5128PD"
      },
      "source": [
        "attributes = ['bill_length_mm','bill_depth_mm','flipper_length_mm']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKfz-5kIugu7"
      },
      "source": [
        "**Normalise the data**\n",
        "- Check: \n",
        "  - i: number of observations\n",
        "  - j: number of attributes \n",
        "\n",
        "- Check: \n",
        "  - Normalisation worked\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRJaWMRHwKxl"
      },
      "source": [
        "data = penguins[attributes].dropna().copy() # select the attributes from the original dataframe & make sure no nans! \n",
        "X = data.to_numpy() # convert them into a numpy array \n",
        "X.shape # check what shape X is "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AShrVWbZBzJ5"
      },
      "source": [
        "XNorm = preprocessing.normalize(X,axis=0,) # normalise each feature\n",
        "XNorm.min(axis=0), XNorm.max(axis=0) # Check data, eg. see that it is between 0 and 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRvYxXqaxx7J"
      },
      "source": [
        "# **Choosing the number of clusters**\n",
        "\n",
        "Regardless of clustering method, we need to pre-assign the number of clusters *n*. \n",
        "\n",
        "By using the penguins dataset, we already know what we are trying to predict and so we have an idea of how many clusters to choose (i.e we know the structure of the data). In many instances, we don't know this already, and so have to estimate how many clusters to use. In these cases, there is no right or wrong answer. There are a few guidance metrics, but it ultimately comes down to you and your knowledge of the data. \n",
        "\n",
        "- I've chosen to predict species - we know there are 3 groups\n",
        "\n",
        "`n = 3 `\n",
        "\n",
        "- Change if you're predicting something else (eg. sex), or just want to see what happens! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaYCRaxz874s"
      },
      "source": [
        "n = 3 # number of clusters "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtK5IUcy41k6"
      },
      "source": [
        "**Metrics for deciding cluster number**\n",
        "\n",
        "There are a lot of metrics to choose from to help inform your cluster number choice. Here are three:\n",
        "- Calinski Harabasz Score: The larger the score the more dense/separated the clusters \n",
        "- Silhouette Score: The higher the score the more well defined the clusters are\n",
        "- Davies-Bouldin Index: Values closer to zero indicate less similarity between clusters \n",
        "\n",
        "Read more about these and others here: https://scikit-learn.org/stable/modules/clustering#clustering-performance-evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ho49Eg3Zc-",
        "cellView": "form"
      },
      "source": [
        "#@markdown Calculate the metrics\n",
        "for k in range(2,6):\n",
        "  kmeanstest = KMeans(n_clusters=k, random_state=0).fit(XNorm)\n",
        "  print('n =',k) \n",
        "  print('Calinski Harabasz Score = ', metrics.calinski_harabasz_score(XNorm, kmeanstest.labels_))\n",
        "  print('Silhouette Score = ', metrics.silhouette_score(XNorm, kmeanstest.labels_, metric='euclidean'))\n",
        "  print('Davies-Bouldin Index = ', metrics.davies_bouldin_score(XNorm, kmeanstest.labels_))\n",
        "  print('  ')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqoafvoC9LLZ"
      },
      "source": [
        "We can see very clearly in this case that three clusters is the optimal number. Be warned however, often these metrics give different answers and are entirely unhelpful (for example see Section 2.5 in https://www.publish.csiro.au/es/pdf/ES20003), hence you often have to decide for yourself. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziqghHco-A7X"
      },
      "source": [
        "#**K-means clustering**\n",
        "\n",
        "K-means aims to minimise the within-cluster sum-of-squares. K-means partitions the observations into *n* clusters using an iterative approach, each time comparing the cluster (or centroid) mean to the previous iteration until little change is found. The result is that each observation is assigned to the cluster with the nearest mean. Read more here: https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
        "\n",
        "In this section we:  \n",
        "- Run through K-means \n",
        "- Look at some of the outputs \n",
        "- Plot \n",
        "\n",
        "**Run K-means** \n",
        "- Very easy on a small data set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K84JFPn_wK2J"
      },
      "source": [
        "kmeans = KMeans(n_clusters=n, random_state=0).fit(XNorm) # Perform clustering "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNTE1TN6vAy9"
      },
      "source": [
        "**Explore some of the outputs**\n",
        "- `cluster_centres_` are the centroid means \n",
        "- `labels_` are the predicted labels [0,1,..,n]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnTo94HerMZA"
      },
      "source": [
        "kmeans.cluster_centers_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi4dso7ozMWO"
      },
      "source": [
        "kmeans.labels_ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv0NfGvLzMdk",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Plot 2D scatter plots**\n",
        "#@markdown - If you are using different predictors to my example, you may have to alter these plots\n",
        "\n",
        "# Add the labels into the dataframe holding the predictors \n",
        "data['k-means species']=kmeans.labels_ \n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
        "sns.scatterplot(data=data,x='flipper_length_mm',y='bill_length_mm',hue='k-means species',palette='flare',ax = axes[0]);\n",
        "axes[0].set_title('Predicted')\n",
        "\n",
        "sns.scatterplot(data=penguins,x='flipper_length_mm',y='bill_length_mm',hue='species',palette='flare',ax = axes[1]);\n",
        "axes[1].set_title('Observed');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9q5tD1rK5fA",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Make a 3D scatter plot** { display-mode: \"both\" }\n",
        "#@markdown - Again - if you have chosen different predictors, you may need to alter\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "ax = plt.axes(projection='3d')\n",
        "p = ax.scatter3D(data['flipper_length_mm'], \n",
        "                 data['bill_length_mm'], \n",
        "                 data['bill_depth_mm'], \n",
        "                 c=data['k-means species'], \n",
        "                 cmap=sns.color_palette('flare',as_cmap=True)); \n",
        "cbar = plt.colorbar(p,drawedges=True,ticks=[0,1,2],boundaries=np.arange(-0.5,3.5,1)); \n",
        "ax.view_init(20, 10) \n",
        "ax.set_xlabel('Flipper Length (mm)');\n",
        "ax.set_ylabel('Bill Length (mm)');\n",
        "ax.set_zlabel('Bill Depth (mm)');\n",
        "plt.title('Predicted penguin data');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we8jDVLIAhw-"
      },
      "source": [
        "# **Evaluate**\n",
        "\n",
        "To simply evaluate how well the k-means model has performed we can look at: \n",
        "- The means of the observed vs predicted clusters\n",
        "- Plot a confusion matrix\n",
        "\n",
        "\n",
        "*For each of these metrics, we need to make sure the labels refer to the same species!*\n",
        "\n",
        "***Note** These evaluation techniques are only appropriate if you know the right anwser. In many applications of clustering, you will not, and you will have to rely on your knowlege of the data (i.e. its physical meaning) to understand if the clustering makes sense (eg. as in https://www.publish.csiro.au/es/pdf/ES20003) \n",
        "\n",
        "**Means** \n",
        "- Using the Pandas DataFrames makes this really easy\n",
        "- If you have changed the clustering method - you will need to check that the species names are assigned correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Xu2KLpaIk7",
        "cellView": "form"
      },
      "source": [
        "#@markdown Calculate means & differences\n",
        "\n",
        "mns = data.groupby('k-means species').mean()[attributes]  # need to assign the correct species to the predicted labels\n",
        "mns.index = ['Adelie','Gentoo','Chinstrap']             # if you have changed the clustering method, you will need to adjust this\n",
        "print('Predicted')\n",
        "print(mns)                                              # print the predicted means\n",
        "print(' ')\n",
        "print('Observed')\n",
        "print(penguins.groupby('species').mean()[attributes])     # print the observed means for each species \n",
        "print(' ')\n",
        "print('Differences')\n",
        "print(penguins.groupby('species').mean()[attributes]-mns) # print the differences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7w4hC52fJdS"
      },
      "source": [
        "**Confusion matrix**\n",
        "- A confusion matrix shows the number of correctly and incorrectly assigned labels, for each different category. \n",
        "- A perfectly predicted data set would result in a diagonal line!\n",
        "- We need to make sure that the 0,1,2s of the predicted model refer to the same as that for the observed. If you've altered the attributes/cluster numbers - this may need to be changed! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Y07YxdAghd",
        "cellView": "form"
      },
      "source": [
        "#@markdown Plot confusion matrix\n",
        "# Get the observed species \n",
        "attributes2 = attributes.copy()\n",
        "attributes2.append('species')\n",
        "obs = penguins[attributes2].dropna().copy()['species']\n",
        "\n",
        "# And the predicted species - again, if the you've changed the code - check! \n",
        "pred = data['k-means species'].copy()\n",
        "pred[pred==0] = 'Adelie'\n",
        "pred[pred==1] = 'Gentoo'\n",
        "pred[pred==2] = 'Chinstrap'\n",
        "\n",
        "# make the confustion matrix\n",
        "cm = metrics.confusion_matrix(y_true=obs, y_pred=pred)\n",
        "\n",
        "# Plot it\n",
        "fig, axes = plt.subplots(1,1,figsize=(10,4))\n",
        "plt.imshow(cm,interpolation='none',cmap='Blues')\n",
        "for (i, j), z in np.ndenumerate(cm):\n",
        "    plt.text(j, i, z, ha='center', va='center')\n",
        "plt.xlabel(\"K-means\")\n",
        "plt.ylabel(\"Observed\")\n",
        "plt.xticks([0,1,2])\n",
        "plt.yticks([0,1,2])\n",
        "axes.set_xticklabels(['A','G','C'])\n",
        "axes.set_yticklabels(['A','G','C']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2xUlMOUI4hv"
      },
      "source": [
        "# **Hierarchical - Agglomerative Clustering** (in brief)\n",
        "\n",
        "We can re-do the clusters using a range of different agglomerative clustering methods. Agglomerative clustering is a bottom up approach, where each observations is it's own cluster to start with and at each iteration may be merged with another, until there is only one cluster. \n",
        "\n",
        "This can be of benefit for data in which there is observations-connectivity, eg. spatial data. You can see an example of agglomerative clustering in this paper: https://www.publish.csiro.au/es/pdf/ES20003\n",
        "\n",
        "There are a number of agglomerative clustering methods, which use different 'linkages', i.e. what they try to minimize to form the clusters. You can read about them here: https://scikit-learn.org/stable/modules/clustering#hierarchical-clustering\n",
        "\n",
        "In this notebook, we will just look at the Ward linkage method, which is very similar to that of k-means in that it minimizes the within cluster sum of squares.\n",
        "\n",
        "Here we will: \n",
        "- Look at the structure of the data to help understand how many clusters we should choose\n",
        "- Compare the to k-means\n",
        "- I've chosen to look at Ward linkage, but you can test the other methods! \n",
        "\n",
        "```\n",
        "link = 'ward'\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8uLaHi2yQPZ"
      },
      "source": [
        "link = 'ward' # or try: 'average', 'single' or 'complete'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IckcquUaJuXC",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Dendrograms** { display-mode: \"both\" }\n",
        "#@markdown - Dendrograms tell us about the structure of the data and how the clusters are merged for the given method \n",
        "#@markdown - Where there are large 'distances' (along y-axis) between merges of clusters implies less similarity between clusters\n",
        "\n",
        "# Function\n",
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "    ''' This was taken from:\n",
        "    https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
        "    '''\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)\n",
        "\n",
        "\n",
        "# create model \n",
        "model = AgglomerativeClustering(linkage=link,n_clusters=None,distance_threshold=0).fit(XNorm)\n",
        "\n",
        "# plot\n",
        "plot_dendrogram(model)\n",
        "plt.title('{} linkage dendrogram'.format(link));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYwEkhEgJuQV",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Agglomerative clustering**\n",
        "#@markdown - Compare to k-means and obs\n",
        "\n",
        "fig, axes = plt.subplots(1,3,figsize=(14,4))\n",
        "\n",
        "# run the agglomerate model\n",
        "agglom = AgglomerativeClustering(linkage=link,n_clusters=n).fit(XNorm)\n",
        "data['{} species'.format(link)] = agglom.labels_\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(x=X[:,1],y=X[:,0],hue=data['{} species'.format(link)],\n",
        "                palette='flare',ax = axes[0]);\n",
        "axes[0].set_title(link)\n",
        "\n",
        "sns.scatterplot(x=X[:,1],y=X[:,0],hue=data['k-means species'],\n",
        "                palette='flare',ax = axes[1]);\n",
        "axes[1].set_title('k-means')\n",
        "\n",
        "sns.scatterplot(data=penguins,x='bill_depth_mm',y='bill_length_mm',hue='species',\n",
        "                palette='flare',ax = axes[2]);\n",
        "axes[2].set_title('observed');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHVhc3AdwWPc"
      },
      "source": [
        "#Acknowledgments: \n",
        "\n",
        "- [Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
        "- [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/)\n",
        "- Hastie, T., Friedman, J., and Tibshirani, R. (2009). [The Elements of Statistical Learning]( https://web.stanford.edu/~hastie/ElemStatLearn/). Springer Series in Statistics. (Springer: New York, NY.)\n",
        "- Kate Saunders (QUT - statistics master)\n",
        "\n"
      ]
    }
  ]
}